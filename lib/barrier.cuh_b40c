#ifndef _BARRIER_H_
#define _BARRIER_H_

#include "util.h"

class GlobalBarrier
{
	public:

		typedef int SyncFlag;

	protected :


		// Counters in global device memory
		SyncFlag* d_sync;

		/**
		 * Simple wrapper for returning a CG-loaded SyncFlag at the specified pointer
		 */
		__device__ __forceinline__ SyncFlag ThreadLoad(int *ptr) const
		{
			int retval;           
			asm volatile ("ld.global.cg.s32 %0, [%1];" :    \
					"=r"(retval) :                        \
					"l" (ptr) );                          
			return retval;  
		}

	public:

		/**
		 * Constructor
		 */
		GlobalBarrier() : d_sync(NULL) {}


		/**
		 * Synchronize
		 */
		__device__ __forceinline__ void sync_grid_opt() const
		{
			volatile SyncFlag *d_vol_sync = d_sync;

			// Threadfence and syncthreads to make sure global writes are visible before
			// thread-0 reports in with its sync counter
			__threadfence();
			__syncthreads();

			if (blockIdx.x == 0) {

				// Report in ourselves
				if (threadIdx.x == 0) {
					d_vol_sync[blockIdx.x] = 1;
				}

				__syncthreads();

				// Wait for everyone else to report in
				for (int peer_block = threadIdx.x; peer_block < gridDim.x; peer_block += blockDim.x) {
					while (ThreadLoad(d_sync + peer_block) == 0) {
						__threadfence_block();
					}
				}

				__syncthreads();

				// Let everyone know it's safe to read their prefix sums
				for (int peer_block = threadIdx.x; peer_block < gridDim.x; peer_block += blockDim.x) {
					d_vol_sync[peer_block] = 0;
				}

			} else {

				if (threadIdx.x == 0) {
					// Report in
					d_vol_sync[blockIdx.x] = 1;

					// Wait for acknowledgement
					while (ThreadLoad(d_sync + blockIdx.x) == 1) {
						__threadfence_block();
					}
				}

				__syncthreads();
			}
		}
};


/**
 * Version of global barrier with storage lifetime management.
 *
 * We can use this in host enactors, and pass the base GlobalBarrier
 * as parameters to kernels.
 */
class Barrier : public GlobalBarrier
{
	protected:

		// Number of bytes backed by d_sync
		size_t sync_bytes;

	public:

		/**
		 * Constructor
		 */
		Barrier(int grid_size) 
			: GlobalBarrier()
		{
			Setup(grid_size);
		}


		/**
		 * Deallocates and resets the progress counters
		 */
		cudaError_t HostReset()
		{
			printf("Destructor\n");
			cudaError_t retval = cudaSuccess;
			if (d_sync) {
				H_ERR(cudaFree(d_sync));
				d_sync = NULL;
			}
			sync_bytes = 0;
			return retval;
		}


		/**
		 * Destructor
		 */
		virtual ~Barrier()
		{
			//HostReset();
		}

		/**
		 * Sets up the progress counters for the next kernel launch (lazily
		 * allocating and initializing them if necessary)
		 */
		void Setup(int sweep_grid_size)
		{
			size_t new_sync_bytes = sweep_grid_size * sizeof(SyncFlag);
			sync_bytes = new_sync_bytes;
			H_ERR(cudaMalloc((void**) &d_sync, sync_bytes));
			cudaMemset(d_sync, 0, sweep_grid_size);
		}

};
#endif
